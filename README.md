# Data-Analytics_2025_BITSoM
# FlexiMart Data Architecture Project

**Student Name:** Sivadutta Pradhan
**Student ID:** BITSOM_BA_DM2729294
**Email:** Solanki.sg2710@gmail.com
**Date:** 04-03-2026

## Project Overview

This project implements a comprehensive data architecture for FlexiMart, an e-commerce company. The solution includes building a Python-based ETL Pipeline to clean messy CSV data and load it into a normalized MySQL database (OLTP), analyzing NoSQL (MongoDB) for flexible product catalog management, and designing a Star Schema Data Warehouse (OLAP) to perform historical sales analysis and reporting.

## Repository Structure
├── part1-database-etl/             # OLTP & ETL
│   ├── etl_pipeline.py             # Main Python ETL Script
│   ├── schema_documentation.md     # 3NF Schema Documentation
│   ├── business_queries.sql        # SQL Solutions for Task 1.3
│   └── data_quality_report.txt     # Auto-generated by ETL script
│   └── sales_raw.csv               # Raw Input Data
│   └── customers_raw.csv           # Raw Input Data
│   └── products_raw.csv            # Raw Input Data
├── part2-nosql/                    # MongoDB Analysis
│   ├── nosql_analysis.md           # Theory Report
│   ├── mongodb_operations.py       # Python script for MongoDB tasks
│   └── products_catalog.json       # JSON Sample Data
│   └── requirements.txt
├── part3-datawarehouse/            # OLAP & Star Schema
│   ├── star_schema_design.md       # Schema Design & Decisions
│   ├── warehouse_schema.sql        # DDL for Data Warehouse
│   ├── warehouse_data.sql          # Data population (Manual + Recursive CTE)
│   └── analytics_queries.sql       # SQL Solutions for Task 3.3
└── README.md                       # Project Documentation
└── .gitignore                      # Hides .env and system files  

## Technologies Used

- Python 3.x, pandas, mysql-connector-python,pymongo
- MySQL 8.0 
- MongoDB 6.0
- Git, VS Code


## Setup Instructions
Install the required Python libraries:
pip install pandas mysql-connector-python python-dotenv pymongo


### Database Setup

```bash
# Create databases
mysql -u root -p -e "CREATE DATABASE fleximart;"
mysql -u root -p -e "CREATE DATABASE fleximart_dw;"

# Run Part 1 - ETL Pipeline
python part1-database-etl/etl_pipeline.py

# Run Part 1 - Business Queries
mysql -u root -p fleximart < part1-database-etl/business_queries.sql

# Run Part 3 - Data Warehouse
mysql -u root -p fleximart_dw < part3-datawarehouse/warehouse_schema.sql
mysql -u root -p fleximart_dw < part3-datawarehouse/warehouse_data.sql
mysql -u root -p fleximart_dw < part3-datawarehouse/analytics_queries.sql


### MongoDB Setup

mongosh < part2-nosql/mongodb_operations.js

**Key Learnings**

1. Data Cleaning Complexity: Learned how to handle inconsistent data formats (e.g., mixed dates like DD-MM-YYYY and MM-DD-YYYY) and standardized phone numbers using Regex.

2. OLTP vs. OLAP: Understood the distinction between designing for transaction consistency (3NF) vs. designing for query performance (Star Schema with Denormalization).

3. Surrogate Keys: Learned the importance of using surrogate keys in Data Warehousing to decouple analytical data from operational system changes.

4. NoSQL Flexibility: Gained hands-on experience with MongoDB's document model, specifically how embedding reviews eliminates expensive joins required in SQL.

## Challenges Faced

1. Missing Date Values:

Issue: The raw sales data contained records with missing or invalid transaction dates, which caused logic errors when grouping orders.

Solution: Implemented a strict filtering mechanism in Pandas to drop rows with invalid dates before attempting to create Order headers, ensuring Referential Integrity.

2. Handling "Unknown" Data:

Issue: Some customers had missing emails, but the database required a UNIQUE constraint.

Solution: Implemented logic to generate unique placeholder emails (e.g., unknown_C003) instead of dropping the customers, preserving the sales history associated with them.

3. String vs. Integer IDs:

Issue: The requirement was to preserve IDs like C001 and P001, but standard auto-increment keys are integers.

Solution: Adjusted the database schema to use INT primary keys for the OLTP system and updated the Python ETL logic to format IDs while keeping them as integers.

